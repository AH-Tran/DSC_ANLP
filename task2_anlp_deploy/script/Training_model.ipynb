{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IAejFervoIl",
        "outputId": "25258fa4-3cc1-444b-801a-7bcc8b928201"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jan 19 11:21:28 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   66C    P0    31W /  70W |  15102MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txTrZXJCJRkZ",
        "outputId": "a5ec7784-e03c-4519-e910-9cf5436a023b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: farm==0.7.1 in /usr/local/lib/python3.8/dist-packages (0.7.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from farm==0.7.1) (0.1.97)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from farm==0.7.1) (0.38.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from farm==0.7.1) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from farm==0.7.1) (2.28.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from farm==0.7.1) (1.7.3)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.8/dist-packages (from farm==0.7.1) (1.26.52)\n",
            "Requirement already satisfied: flask-restplus in /usr/local/lib/python3.8/dist-packages (from farm==0.7.1) (0.13.0)\n",
            "Requirement already satisfied: dotmap in /usr/local/lib/python3.8/dist-packages (from farm==0.7.1) (1.3.30)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.8/dist-packages (from farm==0.7.1) (1.2.2)\n",
            "Requirement already satisfied: torch<1.8,>1.5 in /usr/local/lib/python3.8/dist-packages (from farm==0.7.1) (1.7.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from farm==0.7.1) (0.3.6)\n",
            "Requirement already satisfied: transformers==4.1.1 in /usr/local/lib/python3.8/dist-packages (from farm==0.7.1) (4.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from farm==0.7.1) (57.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from farm==0.7.1) (5.4.8)\n",
            "Requirement already satisfied: flask-cors in /usr/local/lib/python3.8/dist-packages (from farm==0.7.1) (3.0.10)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.8/dist-packages (from farm==0.7.1) (1.1.4)\n",
            "Requirement already satisfied: mlflow<=1.13.1 in /usr/local/lib/python3.8/dist-packages (from farm==0.7.1) (1.13.1)\n",
            "Requirement already satisfied: Werkzeug==0.16.1 in /usr/local/lib/python3.8/dist-packages (from farm==0.7.1) (0.16.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.8/dist-packages (from farm==0.7.1) (0.0.post1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.1.1->farm==0.7.1) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.1.1->farm==0.7.1) (3.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from transformers==4.1.1->farm==0.7.1) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers==4.1.1->farm==0.7.1) (21.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers==4.1.1->farm==0.7.1) (0.0.53)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.8/dist-packages (from transformers==4.1.1->farm==0.7.1) (0.9.4)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.8/dist-packages (from mlflow<=1.13.1->farm==0.7.1) (0.4)\n",
            "Requirement already satisfied: alembic<=1.4.1 in /usr/local/lib/python3.8/dist-packages (from mlflow<=1.13.1->farm==0.7.1) (1.4.1)\n",
            "Requirement already satisfied: gitpython>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from mlflow<=1.13.1->farm==0.7.1) (3.1.30)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from mlflow<=1.13.1->farm==0.7.1) (6.0)\n",
            "Requirement already satisfied: querystring-parser in /usr/local/lib/python3.8/dist-packages (from mlflow<=1.13.1->farm==0.7.1) (1.2.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from mlflow<=1.13.1->farm==0.7.1) (7.1.2)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.8/dist-packages (from mlflow<=1.13.1->farm==0.7.1) (0.4.3)\n",
            "Requirement already satisfied: docker>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from mlflow<=1.13.1->farm==0.7.1) (6.0.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from mlflow<=1.13.1->farm==0.7.1) (2.8.2)\n",
            "Requirement already satisfied: gunicorn in /usr/local/lib/python3.8/dist-packages (from mlflow<=1.13.1->farm==0.7.1) (20.1.0)\n",
            "Requirement already satisfied: azure-storage-blob>=12.0.0 in /usr/local/lib/python3.8/dist-packages (from mlflow<=1.13.1->farm==0.7.1) (12.14.1)\n",
            "Requirement already satisfied: databricks-cli>=0.8.7 in /usr/local/lib/python3.8/dist-packages (from mlflow<=1.13.1->farm==0.7.1) (0.17.4)\n",
            "Requirement already satisfied: prometheus-flask-exporter in /usr/local/lib/python3.8/dist-packages (from mlflow<=1.13.1->farm==0.7.1) (0.21.0)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.8/dist-packages (from mlflow<=1.13.1->farm==0.7.1) (1.4.46)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.8/dist-packages (from mlflow<=1.13.1->farm==0.7.1) (3.19.6)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from mlflow<=1.13.1->farm==0.7.1) (2.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from mlflow<=1.13.1->farm==0.7.1) (1.3.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from mlflow<=1.13.1->farm==0.7.1) (1.15.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->farm==0.7.1) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->farm==0.7.1) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->farm==0.7.1) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->farm==0.7.1) (2.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch<1.8,>1.5->farm==0.7.1) (4.4.0)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from boto3->farm==0.7.1) (0.6.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from boto3->farm==0.7.1) (1.0.1)\n",
            "Requirement already satisfied: botocore<1.30.0,>=1.29.52 in /usr/local/lib/python3.8/dist-packages (from boto3->farm==0.7.1) (1.29.52)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.8/dist-packages (from flask->farm==0.7.1) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.8/dist-packages (from flask->farm==0.7.1) (2.11.3)\n",
            "Requirement already satisfied: aniso8601>=0.82 in /usr/local/lib/python3.8/dist-packages (from flask-restplus->farm==0.7.1) (9.0.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.8/dist-packages (from flask-restplus->farm==0.7.1) (4.3.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.8/dist-packages (from flask-restplus->farm==0.7.1) (2022.7)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.8/dist-packages (from seqeval->farm==0.7.1) (1.0.2)\n",
            "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.8/dist-packages (from alembic<=1.4.1->mlflow<=1.13.1->farm==0.7.1) (1.0.4)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.8/dist-packages (from alembic<=1.4.1->mlflow<=1.13.1->farm==0.7.1) (1.2.4)\n",
            "Requirement already satisfied: azure-core<2.0.0,>=1.24.2 in /usr/local/lib/python3.8/dist-packages (from azure-storage-blob>=12.0.0->mlflow<=1.13.1->farm==0.7.1) (1.26.2)\n",
            "Requirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.8/dist-packages (from azure-storage-blob>=12.0.0->mlflow<=1.13.1->farm==0.7.1) (39.0.0)\n",
            "Requirement already satisfied: msrest>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from azure-storage-blob>=12.0.0->mlflow<=1.13.1->farm==0.7.1) (0.7.1)\n",
            "Requirement already satisfied: pyjwt>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from databricks-cli>=0.8.7->mlflow<=1.13.1->farm==0.7.1) (2.6.0)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.8/dist-packages (from databricks-cli>=0.8.7->mlflow<=1.13.1->farm==0.7.1) (0.8.10)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from databricks-cli>=0.8.7->mlflow<=1.13.1->farm==0.7.1) (3.2.2)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.8/dist-packages (from docker>=4.0.0->mlflow<=1.13.1->farm==0.7.1) (1.4.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from gitpython>=2.1.0->mlflow<=1.13.1->farm==0.7.1) (4.0.10)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from Jinja2<3.0,>=2.10.1->flask->farm==0.7.1) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->transformers==4.1.1->farm==0.7.1) (3.0.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval->farm==0.7.1) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval->farm==0.7.1) (3.1.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from sqlalchemy->mlflow<=1.13.1->farm==0.7.1) (2.0.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->flask-restplus->farm==0.7.1) (5.10.2)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->flask-restplus->farm==0.7.1) (22.2.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->flask-restplus->farm==0.7.1) (0.19.3)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.8/dist-packages (from prometheus-flask-exporter->mlflow<=1.13.1->farm==0.7.1) (0.15.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.8/dist-packages (from cryptography>=2.1.4->azure-storage-blob>=12.0.0->mlflow<=1.13.1->farm==0.7.1) (1.15.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow<=1.13.1->farm==0.7.1) (5.0.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema->flask-restplus->farm==0.7.1) (3.11.0)\n",
            "Requirement already satisfied: isodate>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from msrest>=0.7.1->azure-storage-blob>=12.0.0->mlflow<=1.13.1->farm==0.7.1) (0.6.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from msrest>=0.7.1->azure-storage-blob>=12.0.0->mlflow<=1.13.1->farm==0.7.1) (1.3.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob>=12.0.0->mlflow<=1.13.1->farm==0.7.1) (2.21)\n"
          ]
        }
      ],
      "source": [
        "pip install farm==0.7.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import torch\n",
        "import os\n",
        "import pprint\n",
        "from pathlib import Path\n",
        "\n",
        "from farm.utils import set_all_seeds, MLFlowLogger, initialize_device_settings\n",
        "from farm.modeling.tokenization import Tokenizer\n",
        "from farm.data_handler.utils import write_squad_predictions\n",
        "from farm.modeling.optimization import initialize_optimizer\n",
        "from farm.train import Trainer\n",
        "from farm.modeling.prediction_head import QuestionAnsweringHead\n",
        "from farm.modeling.language_model import LanguageModel\n",
        "from farm.data_handler.processor import SquadProcessor\n",
        "from farm.data_handler.data_silo import DataSilo, DataSiloForCrossVal\n",
        "from farm.modeling.adaptive_model import AdaptiveModel\n",
        "from farm.infer import QAInferencer\n",
        "from farm.eval import Evaluator\n",
        "from farm.evaluation.metrics import metrics_per_bin"
      ],
      "metadata": {
        "id": "6YslawHoJUei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########################\n",
        "########## Settings\n",
        "##########################\n",
        "\n",
        "\n",
        "set_all_seeds(seed=42)\n",
        "device, n_gpu = initialize_device_settings(use_cuda=True)\n",
        "\n",
        "# Setting the path to the desired transformer model\n",
        "lang_model = \"deepset/roberta-base-squad2\"\n",
        "\n",
        "# Setting do_lower_case = False, because used model is cased\n",
        "do_lower_case = False\n",
        "batch_size = 24 # 80\n",
        "n_epochs = 5 \n",
        "\n",
        "# Setting path for input files\n",
        "data_dir = Path(\"data\")\n",
        "\n",
        "# Due to hardware limitations crossvalidation could not be used\n",
        "\n",
        "#save_per_fold_results = False # unsupported for now crossvalidation\n",
        "# n_epochs = 2  crossvalidation\n",
        "# learning_rate = 3e-5 crossvalidation\n",
        "#xval_folds = 5 #for k-fold\n",
        "#dev_split = 0 #for k-fold\n",
        "#evaluate_every = 0 #for k-fold\n",
        "#no_ans_boost = 0 # use large negative values to disable giving \"no answer\" option for k-fold\n",
        "#use_amp = None # for k-fold\n",
        "\n",
        "# Create variables for the training and validation dataset file names\n",
        "train_filename = \"train_squad_format_new.json\"  \n",
        "dev_filename = \"val_squad_format_new.json\"\n",
        "\n",
        "# Setting the amount of expected answers by the system\n",
        "\n",
        "accuracy_at = 10 # accuracy at n is useful for answers inside long documents"
      ],
      "metadata": {
        "id": "Y841Kz_DJWD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.Create a tokenizer\n",
        "tokenizer = Tokenizer.load(\n",
        "pretrained_model_name_or_path=lang_model,\n",
        "do_lower_case=do_lower_case)\n",
        "\n",
        "# 2. Create a DataProcessor that handles all the conversion from raw text into a pytorch Dataset\n",
        "processor = SquadProcessor(\n",
        "tokenizer=tokenizer,\n",
        "max_seq_len=384, #  Samples are truncated after this many tokens.\n",
        "label_list=[\"start_token\", \"end_token\"],\n",
        "metric=\"squad\", # name of metric that shall be used for evaluation, can be “squad” or “top_n_accuracy”\n",
        "train_filename=train_filename,\n",
        "dev_filename=dev_filename,\n",
        "data_dir=data_dir,\n",
        "doc_stride=192, # When the document containing the answer is too long it gets split into part, strided by doc_stride\n",
        ")\n",
        "\n",
        "# 3. Create a DataSilo that loads several datasets (train/dev/test), provides DataLoaders for them and calculates a few descriptive statistics of our datasets\n",
        "data_silo = DataSilo(\n",
        "processor=processor,\n",
        "batch_size=batch_size, distributed=False) \n",
        "\n",
        "# Parameters and their descriptions from the official farm documentation\n",
        "\n",
        "#batch_size (int) – The size of batch that should be returned by the DataLoader for the training set.\n",
        "#eval_batch_size (int) – The size of batch that should be returned by the DataLoaders for the dev and test set.\n",
        "#distributed (bool) – Set to True if you are running in a distributed evn, e.g. using DistributedDataParallel. The DataSilo will init the DataLoader with a DistributedSampler() to distribute batches. \n",
        "#automatic_loading (bool) – Set to False, if you don’t want to automatically load data at initialization\n",
        "#max_multiprocessing_chunksize (int) – max possible value for chunksize as calculated by calc_chunksize() in farm.utils. For certain cases like lm_finetuning, a smaller value can be set, as the default chunksize values are rather large that might cause memory issues.\n",
        "#max_processes (int) – the maximum number of processes to spawn in the multiprocessing.Pool used in DataSilo. It can be set to 1 to disable the use of multiprocessing or make debugging easier.\n",
        "#caching (bool) – save the processed datasets on disk to save time/compute if the same train data is used to run multiple experiments. Each cache has a checksum based on the train_filename of the Processor and the batch size.\n",
        "#cache_path (Path) – root dir for storing the datasets’ cache.\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcETSDDkJlmf",
        "outputId": "9e74455c-b6d3-4115-bc12-75c4bd6f80cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rPreprocessing Dataset data/train_squad_format_new.json:   0%|          | 0/2962 [00:00<?, ? Dicts/s]WARNING:farm.data_handler.processor:Answer using start/end indices is 'Cetaphil cleanser and moisturizer.' while gold label text is ' Cetaphil cleanser and moisturizer'.\n",
            "Example will not be converted for training/evaluation.\n",
            "Preprocessing Dataset data/train_squad_format_new.json:  10%|█         | 297/2962 [00:03<00:27, 97.85 Dicts/s]WARNING:farm.data_handler.processor:Answer using start/end indices is '1. Bad sleep h' while gold label text is '5. Your phone '.\n",
            "Example will not be converted for training/evaluation.\n",
            "Preprocessing Dataset data/train_squad_format_new.json: 100%|██████████| 2962/2962 [00:11<00:00, 268.15 Dicts/s]\n",
            "ERROR:farm.data_handler.processor:Unable to convert 2 samples to features. Their ids are : 649-0-0, 505-0-0\n",
            "Preprocessing Dataset data/val_squad_format_new.json:  41%|████      | 272/671 [00:01<00:01, 259.51 Dicts/s]WARNING:farm.data_handler.processor:Answer using start/end indices is 'divided Democratic Party. Pa' while gold label text is 'a divided Democratic Party. '.\n",
            "Example will not be converted for training/evaluation.\n",
            "Preprocessing Dataset data/val_squad_format_new.json: 100%|██████████| 671/671 [00:02<00:00, 279.28 Dicts/s]\n",
            "ERROR:farm.data_handler.processor:Unable to convert 1 samples to features. Their ids are : 284-0-5\n",
            "Preprocessing Dataset data/val_squad_format_new.json:  41%|████      | 272/671 [00:01<00:01, 292.90 Dicts/s]WARNING:farm.data_handler.processor:Answer using start/end indices is 'divided Democratic Party. Pa' while gold label text is 'a divided Democratic Party. '.\n",
            "Example will not be converted for training/evaluation.\n",
            "Preprocessing Dataset data/val_squad_format_new.json: 100%|██████████| 671/671 [00:02<00:00, 277.67 Dicts/s]\n",
            "ERROR:farm.data_handler.processor:Unable to convert 1 samples to features. Their ids are : 284-0-5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "language_model = LanguageModel.load(lang_model)\n",
        "# b) and a prediction head on top that is suited for our task => Question Answering\n",
        "prediction_head = QuestionAnsweringHead()\n",
        "    \n",
        "model = AdaptiveModel(\n",
        "        language_model=language_model,   \n",
        "        prediction_heads=[prediction_head],\n",
        "        embeds_dropout_prob=0.1,   #The probability that a value in the embeddings returned by the language model will be zeroed. \n",
        "        lm_output_types=[\"per_token\"], #How to extract the embeddings from the final layer of the language model. When set to “per_token”, one embedding will be extracted per input token.\n",
        "                                       # If set to “per_sequence”, a single embedding will be extracted to represent the full input sequence. Can either be a single string, or a list of strings, one for each prediction head.\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "# Parameters and their descriptions from the official farm documentation\n",
        "\n",
        "#loss_aggregation_fn (function) – Function to aggregate the loss of multiple prediction heads. \n",
        "#Input: loss_per_head (list of tensors), global_step (int), batch (dict) Output: aggregated loss (tensor) \n",
        "#Default is a simple sum: lambda loss_per_head, global_step=None, batch=None: sum(tensors) However, you can pass more complex functions\n",
        "# that depend on the current step (e.g. for round-robin style multitask learning) or the actual content of the batch (e.g. certain labels) \n",
        "#Note: The loss at this stage is per sample, i.e one tensor of shape (batchsize) per prediction head."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaHzcf77JoAd",
        "outputId": "91eade58-8dd7-47f9-d929-5f1b4676026f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at deepset/roberta-base-squad2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # 5. Create an optimizer\n",
        "model, optimizer, lr_schedule = initialize_optimizer(\n",
        "  model=model,\n",
        "  learning_rate=3e-5, \n",
        "  schedule_opts={\"name\": \"LinearWarmup\", \"warmup_proportion\": 0.2},\n",
        "  n_batches=len(data_silo.loaders[\"train\"]), #number of batches for training\n",
        "  n_epochs=n_epochs, # number of epochs for training\n",
        "  device=device\n",
        ")\n",
        "\n",
        "# Parameters and their descriptions from the official farm documentation\n",
        "\n",
        "# optimizer_opts – Dict to customize the optimizer. Choose any optimizer available from torch.optim, apex.optimizers or transformers.optimization by supplying the class name and the parameters for the constructor. \n",
        "#Examples: 1) AdamW from Transformers (Default): {“name”: “TransformersAdamW”, “correct_bias”: False, “weight_decay”: 0.01} 2) SGD from pytorch: {“name”: “SGD”, “momentum”: 0.0} \n",
        "#3) FusedLAMB from apex: {“name”: “FusedLAMB”, “bias_correction”: True}\n",
        "\n",
        "#schedule_opts – Dict to customize the learning rate schedule. Choose any Schedule from Pytorch or Huggingface’s Transformers by supplying the class name and the parameters needed by the constructor. \n",
        "#If the dict does not contain num_training_steps it will be set by calculating it from n_batches, grad_acc_steps and n_epochs. \n",
        "#Examples: 1) Linear Warmup (Default): {“name”: “LinearWarmup”, “num_warmup_steps”: 0.1 * num_training_steps, “num_training_steps”: num_training_steps} \n",
        "#2) CosineWarmup: {“name”: “CosineWarmup”, “num_warmup_steps”: 0.1 * num_training_steps, “num_training_steps”: num_training_steps} \n",
        "#3) CyclicLR from pytorch: {“name”: “CyclicLR”, “base_lr”: 1e-5, “max_lr”:1e-4, “step_size_up”: 100}"
      ],
      "metadata": {
        "id": "4ugLoLewJqu_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "c5569383-bc8a-4b56-9220-e2edb58f9ffb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-82fd410730fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 5. Create an optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model, optimizer, lr_schedule = initialize_optimizer(\n\u001b[0m\u001b[1;32m      3\u001b[0m  \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m  \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m  \u001b[0mschedule_opts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"LinearWarmup\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"warmup_proportion\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'initialize_optimizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the training parameters to the function\n",
        "\n",
        "trainer = Trainer(\n",
        "      model=model,\n",
        "      optimizer=optimizer,\n",
        "      data_silo=data_silo,\n",
        "      epochs=n_epochs,\n",
        "      n_gpu=n_gpu,\n",
        "      lr_schedule=lr_schedule,\n",
        "      device=device,\n",
        "  )"
      ],
      "metadata": {
        "id": "v0_r0OBmRvIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Starting the training with the choosen parameters\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NT0IqlmrJsPI",
        "outputId": "6a5e2e3e-04c8-452e-ec8f-a2cef5fdb5bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch 0/4 (Cur. train loss: 0.3636):  26%|██▌       | 100/390 [03:13<09:19,  1.93s/it]\n",
            "Evaluating:   0%|          | 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 16/95 [00:10<00:51,  1.54it/s]\u001b[A\n",
            "Evaluating:  34%|███▎      | 32/95 [00:20<00:41,  1.53it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 48/95 [00:31<00:30,  1.53it/s]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 64/95 [00:41<00:20,  1.53it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 95/95 [01:01<00:00,  1.55it/s]\n",
            "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "Train epoch 0/4 (Cur. train loss: 1.1717):  51%|█████▏    | 200/390 [07:22<05:56,  1.88s/it]\n",
            "Evaluating:   0%|          | 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 16/95 [00:10<00:51,  1.54it/s]\u001b[A\n",
            "Evaluating:  34%|███▎      | 32/95 [00:20<00:40,  1.54it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 48/95 [00:31<00:30,  1.55it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 48/95 [00:41<00:30,  1.55it/s]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 64/95 [00:41<00:20,  1.54it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 95/95 [01:01<00:00,  1.56it/s]\n",
            "Train epoch 0/4 (Cur. train loss: 1.4673):  77%|███████▋  | 300/390 [11:30<02:48,  1.87s/it]\n",
            "Evaluating:   0%|          | 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 16/95 [00:10<00:51,  1.54it/s]\u001b[A\n",
            "Evaluating:  34%|███▎      | 32/95 [00:20<00:41,  1.53it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 48/95 [00:31<00:30,  1.53it/s]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 64/95 [00:41<00:20,  1.54it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 95/95 [01:01<00:00,  1.55it/s]\n",
            "Train epoch 0/4 (Cur. train loss: 1.0867): 100%|██████████| 390/390 [15:21<00:00,  2.36s/it]\n",
            "Train epoch 1/4 (Cur. train loss: 0.4039):   3%|▎         | 10/390 [00:18<11:55,  1.88s/it]\n",
            "Evaluating:   0%|          | 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 16/95 [00:10<00:51,  1.54it/s]\u001b[A\n",
            "Evaluating:  34%|███▎      | 32/95 [00:20<00:41,  1.54it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 48/95 [00:31<00:30,  1.53it/s]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 64/95 [00:41<00:20,  1.54it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 95/95 [01:01<00:00,  1.55it/s]\n",
            "Train epoch 1/4 (Cur. train loss: 0.7933):  28%|██▊       | 110/390 [04:28<08:47,  1.88s/it]\n",
            "Evaluating:   0%|          | 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 16/95 [00:10<00:51,  1.53it/s]\u001b[A\n",
            "Evaluating:  34%|███▎      | 32/95 [00:20<00:41,  1.54it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 48/95 [00:31<00:30,  1.54it/s]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 64/95 [00:41<00:20,  1.53it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 95/95 [01:01<00:00,  1.55it/s]\n",
            "Train epoch 1/4 (Cur. train loss: 1.1099):  54%|█████▍    | 210/390 [08:37<05:40,  1.89s/it]\n",
            "Evaluating:   0%|          | 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 16/95 [00:10<00:51,  1.53it/s]\u001b[A\n",
            "Evaluating:  34%|███▎      | 32/95 [00:20<00:41,  1.53it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 48/95 [00:31<00:30,  1.53it/s]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 64/95 [00:41<00:20,  1.54it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 95/95 [01:01<00:00,  1.55it/s]\n",
            "Train epoch 1/4 (Cur. train loss: 0.4415):  79%|███████▉  | 310/390 [12:45<02:30,  1.88s/it]\n",
            "Evaluating:   0%|          | 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 16/95 [00:10<00:51,  1.54it/s]\u001b[A\n",
            "Evaluating:  34%|███▎      | 32/95 [00:20<00:40,  1.54it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 48/95 [00:31<00:30,  1.54it/s]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 64/95 [00:41<00:20,  1.54it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 95/95 [01:01<00:00,  1.55it/s]\n",
            "Train epoch 1/4 (Cur. train loss: 0.7588): 100%|██████████| 390/390 [16:17<00:00,  2.51s/it]\n",
            "Train epoch 2/4 (Cur. train loss: 0.1688):   5%|▌         | 20/390 [00:37<11:34,  1.88s/it]\n",
            "Evaluating:   0%|          | 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 16/95 [00:10<00:51,  1.54it/s]\u001b[A\n",
            "Evaluating:  34%|███▎      | 32/95 [00:20<00:40,  1.54it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 48/95 [00:31<00:30,  1.54it/s]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 64/95 [00:41<00:20,  1.54it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 95/95 [01:01<00:00,  1.55it/s]\n",
            "Train epoch 2/4 (Cur. train loss: 0.4251):  31%|███       | 120/390 [04:46<08:25,  1.87s/it]\n",
            "Evaluating:   0%|          | 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 16/95 [00:10<00:51,  1.54it/s]\u001b[A\n",
            "Evaluating:  34%|███▎      | 32/95 [00:20<00:40,  1.54it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 48/95 [00:31<00:30,  1.54it/s]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 64/95 [00:41<00:20,  1.53it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 95/95 [01:01<00:00,  1.55it/s]\n",
            "Train epoch 2/4 (Cur. train loss: 0.1876):  56%|█████▋    | 220/390 [08:56<05:20,  1.89s/it]\n",
            "Evaluating:   0%|          | 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 16/95 [00:10<00:51,  1.54it/s]\u001b[A\n",
            "Evaluating:  34%|███▎      | 32/95 [00:20<00:41,  1.54it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 48/95 [00:31<00:30,  1.53it/s]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 64/95 [00:41<00:20,  1.53it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 95/95 [01:01<00:00,  1.55it/s]\n",
            "Train epoch 2/4 (Cur. train loss: 0.3213):  82%|████████▏ | 320/390 [13:05<02:11,  1.88s/it]\n",
            "Evaluating:   0%|          | 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 16/95 [00:10<00:51,  1.54it/s]\u001b[A\n",
            "Evaluating:  34%|███▎      | 32/95 [00:20<00:40,  1.54it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 48/95 [00:31<00:30,  1.54it/s]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 64/95 [00:41<00:20,  1.54it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 95/95 [01:01<00:00,  1.55it/s]\n",
            "Train epoch 2/4 (Cur. train loss: 0.5163): 100%|██████████| 390/390 [16:18<00:00,  2.51s/it]\n",
            "Train epoch 3/4 (Cur. train loss: 0.0967):   8%|▊         | 30/390 [00:56<11:14,  1.87s/it]\n",
            "Evaluating:   0%|          | 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 16/95 [00:10<00:51,  1.54it/s]\u001b[A\n",
            "Evaluating:  34%|███▎      | 32/95 [00:20<00:40,  1.54it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 48/95 [00:31<00:30,  1.54it/s]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 64/95 [00:41<00:20,  1.54it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 95/95 [01:01<00:00,  1.55it/s]\n",
            "Train epoch 3/4 (Cur. train loss: 0.1493):  33%|███▎      | 130/390 [05:06<08:11,  1.89s/it]\n",
            "Evaluating:   0%|          | 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 16/95 [00:10<00:51,  1.53it/s]\u001b[A\n",
            "Evaluating:  34%|███▎      | 32/95 [00:20<00:41,  1.53it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 48/95 [00:31<00:30,  1.53it/s]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 64/95 [00:41<00:20,  1.53it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 95/95 [01:01<00:00,  1.55it/s]\n",
            "Train epoch 3/4 (Cur. train loss: 0.1982):  59%|█████▉    | 230/390 [09:15<05:00,  1.88s/it]\n",
            "Evaluating:   0%|          | 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 16/95 [00:10<00:51,  1.53it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 16/95 [00:20<00:51,  1.53it/s]\u001b[A\n",
            "Evaluating:  34%|███▎      | 32/95 [00:20<00:41,  1.53it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 48/95 [00:31<00:30,  1.53it/s]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 64/95 [00:41<00:20,  1.53it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 95/95 [01:01<00:00,  1.55it/s]\n",
            "Train epoch 3/4 (Cur. train loss: 0.2287):  85%|████████▍ | 330/390 [13:25<01:52,  1.87s/it]\n",
            "Evaluating:   0%|          | 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 16/95 [00:10<00:51,  1.54it/s]\u001b[A\n",
            "Evaluating:  34%|███▎      | 32/95 [00:20<00:41,  1.54it/s]\u001b[A\n",
            "Evaluating:  34%|███▎      | 32/95 [00:31<00:41,  1.54it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 48/95 [00:31<00:30,  1.53it/s]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 64/95 [00:41<00:20,  1.53it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 95/95 [01:01<00:00,  1.55it/s]\n",
            "Train epoch 3/4 (Cur. train loss: 0.0515): 100%|██████████| 390/390 [16:19<00:00,  2.51s/it]\n",
            "Train epoch 4/4 (Cur. train loss: 0.0010):  10%|█         | 40/390 [01:14<10:54,  1.87s/it]\n",
            "Evaluating:   0%|          | 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 16/95 [00:10<00:51,  1.54it/s]\u001b[A\n",
            "Evaluating:  34%|███▎      | 32/95 [00:20<00:40,  1.54it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 48/95 [00:31<00:30,  1.54it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 48/95 [00:41<00:30,  1.54it/s]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 64/95 [00:41<00:20,  1.54it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 95/95 [01:01<00:00,  1.55it/s]\n",
            "Train epoch 4/4 (Cur. train loss: 0.0003):  36%|███▌      | 140/390 [05:24<07:47,  1.87s/it]\n",
            "Evaluating:   0%|          | 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 16/95 [00:10<00:51,  1.54it/s]\u001b[A\n",
            "Evaluating:  34%|███▎      | 32/95 [00:20<00:40,  1.54it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 48/95 [00:31<00:30,  1.54it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 48/95 [00:41<00:30,  1.54it/s]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 64/95 [00:41<00:20,  1.54it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 95/95 [01:01<00:00,  1.55it/s]\n",
            "Train epoch 4/4 (Cur. train loss: 0.0023):  62%|██████▏   | 240/390 [09:34<04:40,  1.87s/it]\n",
            "Evaluating:   0%|          | 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 16/95 [00:10<00:51,  1.54it/s]\u001b[A\n",
            "Evaluating:  34%|███▎      | 32/95 [00:20<00:40,  1.54it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 48/95 [00:31<00:30,  1.54it/s]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 64/95 [00:41<00:20,  1.54it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 95/95 [01:01<00:00,  1.56it/s]\n",
            "Train epoch 4/4 (Cur. train loss: 0.0059):  87%|████████▋ | 340/390 [13:43<01:33,  1.87s/it]\n",
            "Evaluating:   0%|          | 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  17%|█▋        | 16/95 [00:10<00:51,  1.54it/s]\u001b[A\n",
            "Evaluating:  34%|███▎      | 32/95 [00:20<00:41,  1.54it/s]\u001b[A\n",
            "Evaluating:  51%|█████     | 48/95 [00:31<00:30,  1.53it/s]\u001b[A\n",
            "Evaluating:  67%|██████▋   | 64/95 [00:41<00:20,  1.53it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 95/95 [01:01<00:00,  1.54it/s]\n",
            "Train epoch 4/4 (Cur. train loss: 0.0009): 100%|██████████| 390/390 [16:19<00:00,  2.51s/it]\n",
            "Evaluating: 100%|██████████| 95/95 [01:01<00:00,  1.55it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AdaptiveModel(\n",
              "  (language_model): Roberta(\n",
              "    (model): RobertaModel(\n",
              "      (embeddings): RobertaEmbeddings(\n",
              "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "        (token_type_embeddings): Embedding(1, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (encoder): RobertaEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0): RobertaLayer(\n",
              "            (attention): RobertaAttention(\n",
              "              (self): RobertaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): RobertaSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): RobertaIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): RobertaOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (1): RobertaLayer(\n",
              "            (attention): RobertaAttention(\n",
              "              (self): RobertaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): RobertaSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): RobertaIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): RobertaOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (2): RobertaLayer(\n",
              "            (attention): RobertaAttention(\n",
              "              (self): RobertaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): RobertaSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): RobertaIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): RobertaOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (3): RobertaLayer(\n",
              "            (attention): RobertaAttention(\n",
              "              (self): RobertaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): RobertaSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): RobertaIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): RobertaOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (4): RobertaLayer(\n",
              "            (attention): RobertaAttention(\n",
              "              (self): RobertaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): RobertaSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): RobertaIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): RobertaOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (5): RobertaLayer(\n",
              "            (attention): RobertaAttention(\n",
              "              (self): RobertaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): RobertaSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): RobertaIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): RobertaOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (6): RobertaLayer(\n",
              "            (attention): RobertaAttention(\n",
              "              (self): RobertaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): RobertaSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): RobertaIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): RobertaOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (7): RobertaLayer(\n",
              "            (attention): RobertaAttention(\n",
              "              (self): RobertaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): RobertaSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): RobertaIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): RobertaOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (8): RobertaLayer(\n",
              "            (attention): RobertaAttention(\n",
              "              (self): RobertaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): RobertaSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): RobertaIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): RobertaOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (9): RobertaLayer(\n",
              "            (attention): RobertaAttention(\n",
              "              (self): RobertaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): RobertaSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): RobertaIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): RobertaOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (10): RobertaLayer(\n",
              "            (attention): RobertaAttention(\n",
              "              (self): RobertaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): RobertaSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): RobertaIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): RobertaOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (11): RobertaLayer(\n",
              "            (attention): RobertaAttention(\n",
              "              (self): RobertaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): RobertaSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): RobertaIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): RobertaOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (pooler): RobertaPooler(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (activation): Tanh()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (prediction_heads): ModuleList(\n",
              "    (0): QuestionAnsweringHead(\n",
              "      (feed_forward): FeedForwardBlock(\n",
              "        (feed_forward): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=2, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # 8. Load pre-trained question-answering model\n",
        "model.connect_heads_with_processor(data_silo.processor.tasks, require_labels=True)\n",
        "# Number of predictions the model will make per Question.\n",
        "# The multiple predictions are used for evaluating top n recall.\n",
        "#model.prediction_heads[0].n_best = accuracy_at\n",
        "model.prediction_heads[0].n_best = 10\n",
        "\n",
        "# 5. The calibration of model confidence scores sets one parameter, which is called temperature and can be accessed through the prediction_head.\n",
        "# This temperature is applied to each logit in the forward pass, where each logit is divided by the temperature.\n",
        "# A softmax function is applied to the logits afterward to get confidence scores in the range [0,1].\n",
        "# A temperature larger than 1 decreases the model’s confidence scores.\n"
      ],
      "metadata": {
        "id": "_lGyss56Jtm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9a. We can either manually set the temperature (default value is 1.0)...\n",
        "model.prediction_heads[0].temperature_for_confidence = torch.nn.Parameter((torch.ones(1) * 1.0).to(device=device))\n",
        "\n",
        "# 9b. ...or we can run the evaluator on the dev set and use it to calibrate confidence scores with a technique called temperature scaling.\n",
        "# It will align the confidence scores with the model's accuracy based on the dev set data by tuning the temperature parameter.\n",
        "# During the calibration, this parameter is automatically set internally as an attribute of the prediction head.\n",
        "evaluator_dev = Evaluator(\n",
        "    data_loader=data_silo.get_data_loader(\"dev\"),\n",
        "    tasks=data_silo.processor.tasks,\n",
        "    device=device\n",
        ")\n",
        "result_dev = evaluator_dev.eval(model, return_preds_and_labels=True, calibrate_conf_scores=True)\n",
        "# evaluator_dev.log_results(result_dev, \"Dev\", logging=False, steps=len(data_silo.get_data_loader(\"dev\"))"
      ],
      "metadata": {
        "id": "XzyWTKAYJyJr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65d29059-23b2-4677-c8b7-1805181e827c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 95/95 [01:02<00:00,  1.52it/s]\n",
            "WARNING:farm.eval:temperature used for calibration of confidence scores changed by more than 111.56253814697266 percent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Optionally, run the evaluator on the test set to see how well the confidence scores are aligned with the model's accuracy\n",
        "evaluator_test = Evaluator(\n",
        "    data_loader=data_silo.get_data_loader(\"test\"),\n",
        "    tasks=data_silo.processor.tasks,\n",
        "    device=device\n",
        ")\n",
        "result_test = evaluator_test.eval(model, return_preds_and_labels=True)[0]\n",
        "em_per_bin, confidence_per_bin, count_per_bin = metrics_per_bin(result_test[\"preds\"], result_test[\"labels\"], num_bins=10)\n",
        "for bin_number in range(10):\n",
        "    print(f\"Bin {bin_number} - exact match: {em_per_bin[bin_number]}, average confidence score: {confidence_per_bin[bin_number]}\")\n"
      ],
      "metadata": {
        "id": "qIAf4jQpJ0Cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "322b0833-8e61-4962-d2eb-54cab00805be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 95/95 [01:02<00:00,  1.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bin 0 - exact match: 0.12359550561797752, average confidence score: 0.056291134986147455\n",
            "Bin 1 - exact match: 0.23469387755102042, average confidence score: 0.1450109962868143\n",
            "Bin 2 - exact match: 0.25, average confidence score: 0.2470808042164304\n",
            "Bin 3 - exact match: 0.2545454545454545, average confidence score: 0.3480545675890012\n",
            "Bin 4 - exact match: 0.28888888888888886, average confidence score: 0.4553984383535054\n",
            "Bin 5 - exact match: 0.23636363636363636, average confidence score: 0.555782878974622\n",
            "Bin 6 - exact match: 0.3230769230769231, average confidence score: 0.6563104570413438\n",
            "Bin 7 - exact match: 0.5609756097560976, average confidence score: 0.7448070222262021\n",
            "Bin 8 - exact match: 0.6428571428571429, average confidence score: 0.8573968206133161\n",
            "Bin 9 - exact match: 0.881578947368421, average confidence score: 0.9323726782673284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Hooray! You have a model with calibrated confidence scores.\n",
        "# Store the model and the temperature parameter will be stored automatically as an attribute of the prediction head.\n",
        "save_dir = Path(\"saved_models/qa-model-task2\")\n",
        "model.save(save_dir)\n",
        "processor.save(save_dir)\n"
      ],
      "metadata": {
        "id": "i6z3heztJ2Zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r content/file.zip saved_models/qa-model-task2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOfqiYc2H5Tu",
        "outputId": "d6d81667-a15e-41a7-c132-7974f9819df0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: saved_models/qa-model-task2/ (stored 0%)\n",
            "  adding: saved_models/qa-model-task2/prediction_head_0_config.json (deflated 43%)\n",
            "  adding: saved_models/qa-model-task2/tokenizer_config.json (deflated 71%)\n",
            "  adding: saved_models/qa-model-task2/prediction_head_0.bin (deflated 17%)\n",
            "  adding: saved_models/qa-model-task2/processor_config.json (deflated 56%)\n",
            "  adding: saved_models/qa-model-task2/special_tokens_map.json (deflated 83%)\n",
            "  adding: saved_models/qa-model-task2/vocab.json (deflated 59%)\n",
            "  adding: saved_models/qa-model-task2/language_model_config.json (deflated 50%)\n",
            "  adding: saved_models/qa-model-task2/language_model.bin (deflated 7%)\n",
            "  adding: saved_models/qa-model-task2/merges.txt (deflated 53%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"content/file.zip\")"
      ],
      "metadata": {
        "id": "sWJKGMBVJ7eE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "9eb3d838-548c-4d7c-dd7c-6f865129f2b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_aea2ec68-e3a7-4281-8d65-64e1095ea537\", \"file.zip\", 461950732)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. When making a prediction with the calibrated model, we could filter out predictions where the model is not confident enough\n",
        "# To this end, load the stored model, which will automatically load the stored temperature parameter.\n",
        "# The confidence scores are automatically adjusted based on this temperature parameter.\n",
        "# For each prediction, we can check the model's confidence and decide whether to output the prediction or not.\n",
        "inferencer = QAInferencer.load(save_dir, batch_size=40, gpu=True, task_type= \"question_answering\")\n",
        "\n",
        "# Parameters and their descriptions from the official farm documentation\n",
        "\n",
        "#max_seq_len (int) – maximum length of one text sample\n",
        "#doc_stride (int) – Only QA: When input text is longer than max_seq_len it gets split into parts, strided by doc_stride\n",
        "#extraction_strategy (str) – Strategy to extract vectors. Choices: ‘cls_token’ (sentence vector), ‘reduce_mean’ (sentence vector), reduce_max (sentence vector), ‘per_token’ (individual token vectors)\n",
        "#extraction_layer (int) – number of layer from which the embeddings shall be extracted. Default: -1 (very last layer).\n",
        "\n",
        "# Testing the trained model\n",
        "QA_input = [\n",
        "    {\n",
        "        \"questions\": [\"Who counted the game among the best ever made?\"],\n",
        "        \"text\": \"Twilight Princess was released to universal critical acclaim and commercial success. It received perfect scores from major publications such as 1UP.com, Computer and Video Games, Electronic Gaming Monthly, Game Informer, GamesRadar, and GameSpy. On the review aggregators GameRankings and Metacritic, Twilight Princess has average scores of 95% and 95 for the Wii version and scores of 95% and 96 for the GameCube version. GameTrailers in their review called it one of the greatest games ever created.\"\n",
        "    }]\n",
        "result = inferencer.inference_from_dicts(dicts=QA_input, return_json=False)[0]\n",
        "#print(result)\n",
        "\n",
        "for i in result.prediction:\n",
        "  print(i.answer, i.confidence)\n",
        "if result.prediction[0].confidence > 0.9:\n",
        "    print(result.prediction[0].answer)\n",
        "else:\n",
        "    print(\"The confidence is not high enough to give an answer.\")"
      ],
      "metadata": {
        "id": "B2iNYAuBKCWs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbc64802-cc02-48ea-b76f-9b56fd45b11b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:farm.modeling.prediction_head:Some unused parameters are passed to the QuestionAnsweringHead. Might not be a problem. Params: {\"training\": false, \"num_labels\": 2, \"ph_output_type\": \"per_token_squad\", \"model_type\": \"span_classification\", \"label_tensor_name\": \"question_answering_label_ids\", \"label_list\": [\"start_token\", \"end_token\"], \"metric\": \"squad\", \"name\": \"QuestionAnsweringHead\"}\n",
            "WARNING:farm.utils:Failed to log params: Changing param values is not allowed. Param with key='lm_name' was already logged with value='deepset/roberta-base-squad2' for run ID='1916b49fe7d546b0b58e9e34b2001f11'. Attempted logging new value 'saved_models/qa-model-task2'.\n",
            "WARNING:farm.utils:ML Logging is turned off. No parameters, metrics or artifacts will be logged to MLFlow.\n",
            "WARNING:farm.data_handler.dataset:Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 24.97 Batches/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Twilight Princess 0.341556578874588\n",
            "GameTrailers 0.37477704882621765\n",
            "Twilight Princess was released to universal critical acclaim and commercial success. It received perfect scores from major publications such as 1UP.com, Computer and Video Games, Electronic Gaming Monthly, Game Informer, GamesRadar, and GameSpy. On the review aggregators GameRankings and Metacritic, Twilight Princess has average scores of 95% and 95 for the Wii version and scores of 95% and 96 for the GameCube version. GameTrailers 0.341556578874588\n",
            "Princess 0.013279247097671032\n",
            "ilight Princess 0.004485561978071928\n",
            "no_answer 0.036037511425092816\n",
            "The confidence is not high enough to give an answer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}